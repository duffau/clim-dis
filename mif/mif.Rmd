---
title: "Iterated filtering: Principles and practice"
author: "Edward Ionides"
date: "06/25/2015"
output:
  html_document:
    theme: flatly
    toc: yes
bibliography: mif.bib
csl: ../ecology.csl

---

Licensed under the Creative Commons attribution-noncommercial license, http://creativecommons.org/licenses/by-nc/3.0/.
Please share and remix noncommercially, mentioning its origin.  
![CC-BY_NC](http://dept.stat.lsa.umich.edu/~ionides/tutorials/cc-by-nc.png)

```{r opts,include=FALSE}
library(pomp)
library(knitr)
prefix <- "mif"
opts_chunk$set(
  progress=TRUE,
  prompt=FALSE,tidy=FALSE,highlight=TRUE,
  strip.white=TRUE,
  warning=FALSE,
  message=FALSE,
  error=FALSE,
  echo=TRUE,
  cache=FALSE,
  results='markup',
  fig.show='asis',
  size='small',
  fig.lp="fig:",
  fig.path=paste0("figure/",prefix,"-"),
  cache.path=paste0("cache/",prefix,"-"),
  fig.pos="h!",
  fig.align='center',
  fig.height=4,fig.width=6.83,
  dpi=300,
  dev='png',
  dev.args=list(bg='transparent')
  )

options(
  pomp.cache="cache",
  keep.source=TRUE,
  encoding="UTF-8"
  )

library(ggplot2)
theme_set(theme_bw())
```


## Objectives

This tutorial covers likelihood estimation via iterated filtering, in preparation for a [series of tutorials](http://dept.stat.lsa.umich.edu/~ionides/tutorials/index.html) on time series analysis using mechanistic modeling. It presupposes familiarity with building partially observed Markov process (POMP) objects in the R package pomp [@king15]. pomp is available from [CRAN](http://cran.r-project.org/web/packages/pomp). The code here runs pomp version `r packageVersion("pomp")`, downloaded from  [R-Forge](http://pomp.r-forge.r-project.org), on R version `r getRversion()`. This tutorial follows on from the topic of carrying out particle filtering (also known as sequential Monte Carlo) via 'pfilter' in pomp. We have the following goals:

1. Review the available options for inference on POMP models, to put iterated filtering in context.

2. Understand how iterated filtering algorithms carry out repeated particle filtering operations, with randomly perturbed parameter values, in order to maximize the likelihood.

3. Gain experience carrying out statistical investigations using iterated filtering in a relatively simple situation (fitting an SIR model to a boarding school flu outbreak).

## Introduction

Many, many statistical methods have been proposed for inference on POMP models [@he10,@king15]. The volume of research indicates both the importance and the difficulty of the problem. Let's start by considering three criteria to categorize inference methods: the plug-and-play property; full-information or feature-based; frequentist or Bayesian.

### Plug-and-play (also called simulation-based) methods

* Inference methodology that calls 'rprocess' but not 'dprocess' is said to be ___plug-and-play__. All popular modern Monte Carlo methods fall into this category. 

* Simulation-based is equivalent to plug-and-play. 
   + Historically, simulation-based meant simulating forward from initial conditions to the end of the time series. 
   + However, particle filtering methods instead consider each observation interval sequentially. They carry out multiple, carefully selected, simulations over each interval.

* We permit plug-and-play methods to call 'dmeasure'. A method that uses only 'rprocess' and 'rmeasure' is called doubly plug-and-play.

* Two __non-plug-and-play__ methods (EM algorithms and MCMC) have theoretical convergence problems for nonlinear POMP models. The failures of these two workhorses of statistical computation have prompted development of alternative methodology.

### Full-information and feature-based methods

* __Full-information__ methods are defined to be those based on the likelihood function for the full data (i.e., likelihood-based frequentist inference and Bayesian inference).

* __Feature-based__ methods either consider a summary statistic (a function of the data) or work with an an alternative to the likelihood.

* Asymptotically, full_information methods are statistically efficient and feature-based methods are not.
   + Loss of statistical efficiency could potentially be an acceptable tradeoff for advantages in computational efficiency.
   + However, good low-dimensional summary statistics can be hard to find. 
   + When using statistically inefficient methods, it can be hard to know how much information you are losing. 
   + Intuition and scientific reasoning can be inadequate tools to derive informative low-dimensional summary statistics [@shrestha11,@ionides11-statSci].

### Bayesian and frequentist methods

* Recently, plug-and-play Bayesian methods have been discovered:
   + particle Markov chain Monte Carlo (PMCMC) [@andrieu10].
   + approximate Bayesian computation (ABC) [@toni09].

* Prior belief specification is both the strength and weakness of Bayesian methodology:
   + The likelihood surface for nonlinear POMP models often contains nonlinear ridges and variations in curvature. 
   + These situations bring into question the appropriateness of independent priors derived from expert opinion on marginal distributions of parameters.
   + They also are problematic for specification of "flat" or "uninformative" prior beliefs.
   + Expert opinion can be treated as data for non-Bayesian analysis. However, our primary task is to identify the information in the data under investigation, so it can be helpful to use methods that do not force us to make our conclusions dependent on quantification of prior beliefs.

### Full-information, plug-and-play, frequentist methods

* Iterated filtering methods [@ionides06,@ionides15] are the only currently available, full-information, plug-and-play, frequentist methods for POMP models.

* Iterated filtering methods have been shown to solve likelihood-based inference problems for epidemiological situations which are computationally intractable for available Bayesian methodology [@ionides15].

### Summary of POMP inference methodologies
                             
                                                                             

------------------------

|  |                 | __Frequentist__        | __Bayesian__        |
| --- | ----------------- | ------------------ | ------------- |
| __Plug-and-play__ | __Full-information__  | iterated filtering | particle MCMC |
| | __Feature-based__     | simulated moments  | ABC           |
| |                 | synthetic likelihood  |                 |
| | | | |
| __Not-plug-and-play__ | __Full-information__  |EM algorithm       | MCMC |
| |                  |Kalman filter      |
| | __Feature-based__     |Yule-Walker        | extended Kalman filter |
| |                   |extended Kalman filter |

-----------------------------------------------

1. Yule-Walker is the method of moments for ARMA, a linear Gaussian POMP.

2. The Kalman filter gives the exact likelihood for a linear Gaussian POMP. The extended Kalman filter gives an approximation for nonlinear models that can be used for quasi-likelihood or quasi-Bayesian inference.


## An iterated filtering algorithm (IF2)

* We use the IF2 algorithm of @ionides15. 

* A particle filter is carried out with the parameter vector for each particle doing a random walk.

* At the end of the time series, the collection of parameter vectors is recycled as starting parameters for a new particle filter with a smaller random walk variance.

* Theoretically, this procedure converges toward the region of parameter space maximizing the maximum likelihood.

* Empirically, we can test this claim on examples.

### IF2 algorithm pseudocode

__model input__:
Simulators for $f_{X_0}(x_0;\theta)$ and $f_{X_n|X_{n-1}}(x_n| x_{n-1}; \theta)$;
evaluator for $f_{Y_n|X_n}(y_n| x_n;\theta)$;
data, $y^*_{1:N}$ 

__algorithmic parameters__:
Number of iterations, $M$;
number of particles, $J$;
initial parameter swarm, $\{\Theta^0_j, j=1,\dots,J\}$;
perturbation density, $h_n(\theta|\varphi;\sigma)$;
perturbation scale, $\sigma_{1{:}M}$ 

__output__:
Final parameter swarm, $\{\Theta^M_j, j=1,\dots,J\}$ 

 
1. $\quad$ For $m$ in $1{:} M$
2. $\quad\quad\quad$ $\Theta^{F,m}_{0,j}\sim h_0(\theta|\Theta^{m-1}_{j}; \sigma_m)$ for $j$ in $1{:} J$
3. $\quad\quad\quad$ $X_{0,j}^{F,m}\sim f_{X_0}(x_0 ; \Theta^{F,m}_{0,j})$ for $j$ in $1{:} J$
4. $\quad\quad\quad$ For $n$ in $1{:} N$
5. $\quad\quad\quad\quad\quad$ $\Theta^{P,m}_{n,j}\sim h_n(\theta|\Theta^{F,m}_{n-1,j},\sigma_m)$ for $j$ in $1{:} J$
6. $\quad\quad\quad\quad\quad$ $X_{n,j}^{P,m}\sim f_{X_n|X_{n-1}}(x_n | X^{F,m}_{n-1,j}; \Theta^{P,m}_j)$ for $j$ in $1{:} J$
7. $\quad\quad\quad\quad\quad$ $w_{n,j}^m = f_{Y_n|X_n}(y^*_n| X_{n,j}^{P,m} ; \Theta^{P,m}_{n,j})$ for $j$ in $1{:} J$
8. $\quad\quad\quad\quad\quad$ Draw $k_{1{:}J}$ with $P[k_j=i]=  w_{n,i}^m\Big/\sum_{u=1}^J w_{n,u}^m$
9.  $\quad\quad\quad\quad\quad$ $\Theta^{F,m}_{n,j}=\Theta^{P,m}_{n,k_j}$ and $X^{F,m}_{n,j}=X^{P,m}_{n,k_j}$ for $j$ in $1{:} J$
10. $\quad\quad\quad$ End For
11. $\quad\quad\quad$ Set $\Theta^{m}_{j}=\Theta^{F,m}_{N,j}$ for $j$ in $1{:} J$
12. $\quad$ End For

__comments__:

* The $N$ loop (lines 4 through 10) is a basic particle filter applied to a model with stochastic perturbations to the parameters.

* The $M$ loop repeats this particle filter with decreasing perturbations.

* The superscript $F$ in $\Theta^{F,m}_{n,j}$ and $X^{F,m}_{n,j}$ denote solutions to the _filtering_ problem, with the particles $j=1,\dots,J$ providing a Monte Carlo representation of the conditional distribution at time $n$ given data $y^*_{1:n}$ for filtering iteration $m$.

* The superscript $P$ in $\Theta^{P,m}_{n,j}$ and $X^{P,m}_{n,j}$ denote solutions to the _prediction_ problem, with the particles $j=1,\dots,J$ providing a Monte Carlo representation of the conditional distribution at time $n$ given data $y^*_{1:n-1}$ for filtering iteration $m$.

* The _weight_ $w^m_{n,j}$ gives the likelihood of the data at time $n$ for particle $j$ in filtering iteration $m$.

### Choosing the algorithmic settings for IF2

* The initial parameter swarm, $\{ \Theta^0_j, j=1,\dots,J\}$, usually consists of $J$ identical replications of some starting parameter vector.

* $J$ is set to be sufficient for particle filtering. By the time of the last iteration ($m=M$) one should not have effective sample size close to 1. 

* Perturbations are usually chosen to be Gaussian, with $\sigma_m$ being a scale factor for iteration $m$:
$$h_n(\theta|\varphi;\sigma) \sim N[\varphi, \sigma^2_m V_n].$$
    + $V_n$ is usually taken to be diagonal,
$$ V_n = \left( \begin{array}{ccccc}
v_{1,n}^2 & 0 & 0 & \rightarrow & 0 \\
0 & v_{2,n}^2 &  0 & \rightarrow & 0 \\
0 & 0 & v_{3,n}^2 & \rightarrow & 0 \\
\downarrow & & & \searrow & \downarrow \\
0 & 0 & 0 & \rightarrow & v_{p,n}^2 \end{array}\right).$$
    + If $\theta_i$ is a parameter that affects the dynamics or observations throughout the timeseries, it is called a __regular parameter__, and it is often appropriate to specify
$$ v_{i,n} = v_i,$$
    + If $\theta_j$ is a parameter that affects only the initial conditions of the dynamic model, it is called an __initial value parameter__ (IVP) and it is appropriate to specify
$$ v_{j,n} = \left\{\begin{array}{ll} v_j & \mbox{if $n=0$} \\
                                      0 & \mbox{if $n>0$} \end{array}\right.$$
    + If $\theta_k$ is a break-point parameter that models how the system changes at time $t_q$ then $\theta_k$ is like an IVP at time $t_q$ and it is appropriate to specify
$$ v_{j,n} = \left\{\begin{array}{ll} v_j & \mbox{if $n=q$} \\
                                      0 & \mbox{if $n\neq q$} \end{array}\right.$$

* $\sigma_{1:M}$ is called a __cooling schedule__, following a thermodynamic analogy popularized by [simulated annealing](https://en.wikipedia.org/wiki/Simulated_annealing). As $\sigma_m$ becomes small, the system cools toward a "freezing point". If the algorithm is working sucessfully, the freezing point should be close to the lowest-energy state of the system, i.e., the MLE.


* It is generally helpful for optimization to provide transformations of the parameters so that (on the estimation scale) they are real-valued and have uncertainty on the order of 1 unit. For example, one typically takes a logarithmic transformation of positive parameters and a logistic transformation of $[0,1]$ valued parameters.
+ On this scale, it is surprisingly often effective to take
$$ v_i = 0.02$$
for regular parameters (RPs) and 
$$ v_j = 0.1$$
for initial value parameters (IVPs).

* We suppose that $\sigma_1=1$, since the scale of the parameters is addressed by the matrix $V_n$ . Early on in an investigation, one might take $M=100$ and $\sigma_M=0.1$. Later on, consideration of diagnostic plots may suggest refinements. 

* It is surprising that useful general advice exists for these quantities that could in principle be highly model-specific.
Here is one possible explanation: the precision of interest is often the second significant figure and there are often order 100 observations (10 monthly obsevations would be too few to fit a mechanistic model; 1000 would be unusual for an epidemiological system). 

## Applying IF2 to a boarding school influenza outbreak

For a relatively simple epidemiological example of IF2, we consider fitting a stochastic SIR model to an influenza outbreak in a British boarding school [@anonymous78]. Reports consist of the number of children confined to bed for each of the 14 days of the outbreak. The total number of children at the school was 763, and a total of 512 children spent time away from class. Only one adult developed influenza-like illness, so adults are omitted from the data and model. First, we read in the boarding school flu (bsflu) data:
```{r load_bbs}
bsflu_data <- read.table("http://kinglab.eeb.lsa.umich.edu/SBIED/data/bsflu_data.txt")
```

Our model is a variation on a basic SIR Markov chain, with state $X(t)=(S(t),I(t),R_1(t),R_2(t),R_3(t) )$ giving the number of individuals in the susceptible and infectious categories, and three stages of recovery. The recovery stages, $R_1$, $R_2$ and $R_3$, are all modeled to be non-contagious. $R_1$ consists of individuals who are bed-confined if they show symptoms; $R_2$ consists of individuals who are convalescent if they showed symptoms; $R_3$ consists of recovered individuals who have returned to school-work if they were symtomatic.  The observation on day $n$ of the observed epidemic (with $n=1$ being 22 January) consists of the numbers of children who are bed-confined and convalescent. These measurements are modeled as $Y_n=(B_n,C_n)$ with $B_n\sim\mathrm{Poisson}(\rho R_1(t_n))$ and $C_n\sim\mathrm{Poisson}(\rho R_2(t_n))$. Here, $\rho$ is a reporting rate corresponding to the chance of being symptomatic.

The index case for the epidemic was proposed to be a boy returning to Britain from Hong Kong, who was reported to have a transient febrile illness
from 15 to 18 January. It would therefore be reasonable to initialize the epidemic with $I(t_0)=1$ at $t_0=-6$. This is a little tricky to reconcile with the rest of the data; for now, we avoid this issue by instead initializing with  $I(t_0)=1$ at $t_0=0$. All other individuals are modeled to be initially susceptible.

Our Markov transmission model is that each individual in $S$ transitions to $I$ at rate $\beta I(t)$; each individual in $I$ transitions at rate $\mu_I$ to $R_1$. Subsequently, the individual moves from $R_1$ to $R_2$ at  rate $\mu_{R_1}$, and finally from $R_2$ to $R_3$ at rate $\mu_{R_2}$. Therefore, $1/\mu_I$ is the mean infectious time prior to bed-confinement; $1/R_1$ is the mean duration of bed-confinement for symptomatic cases;  $1/R_2$ is the mean duration of convalescence for symptomatic cases. All rates have units $\mathrm{day}^{-1}$. 

This model has limitations and weaknesses. Writing down and fitting a model is a starting point for data analysis, not an end point. In particular, one should try model variations. For example, one could include a latency period for infections, or one could modify the model to give a better description of the bed-confinement and convalescence processes. Ten individuals received antibiotics for secondary infections, and they had longer bed-confinement and convalescence times. Partly for this reason, we will initially fit only the bed-confinement data, using $Y_n=B_n$ for our `dmeasure`. 

For the code, we represent the states ($S$, $I$, $R_1$, $R_2$) and the parameters ($\beta$, $\mu_I$, $\rho$, $\mu_{R_1}$, $\mu_{R_2}$) as follows:

```{r bsflu_names}
bsflu_statenames <- c("S","I","R1","R2")
bsflu_paramnames <- c("beta","mu_I","rho","mu_R1","mu_R2")
```

The observation names ($B$, $C$) are the names of the data variables:

```{r bsflu_obsnames}
(bsflu_obsnames <- colnames(bsflu_data)[1:2])
```

We do not need a representation of $R_3$ since the total population size is fixed at $P=763$ and hence $R_3(t)=P-S(t)-I(t)-R_1(t)-R_2(t)$. 
Now, we write the model code:

```{r csnippets_bsflu}
bsflu_dmeasure <- "
  lik = dpois(B,rho*R1+1e-6,give_log);
"

bsflu_rmeasure <- "
  B = rpois(rho*R1+1e-6);
  C = rpois(rho*R2);
"

bsflu_rprocess <- "
  double t1 = rbinom(S,1-exp(-beta*I*dt));
  double t2 = rbinom(I,1-exp(-dt*mu_I));
  double t3 = rbinom(R1,1-exp(-dt*mu_R1));
  double t4 = rbinom(R2,1-exp(-dt*mu_R2));
  S -= t1;
  I += t1 - t2;
  R1 += t2 - t3;
  R2 += t3 - t4;
"

bsflu_fromEstimationScale <- "
 Tbeta = exp(beta);
 Tmu_I = exp(mu_I);
 Trho = expit(rho);
"

bsflu_toEstimationScale <- "
 Tbeta = log(beta);
 Tmu_I = log(mu_I);
 Trho = logit(rho);
"

bsflu_initializer <- "
 S=762;
 I=1;
 R1=0;
 R2=0;
"
```

We can now build the pomp object:

```{r pomp_bsflu}
bsflu <- pomp(
     data=bsflu_data,
     times="day",
     t0=0,
     rprocess=euler.sim(
       step.fun=Csnippet(bsflu_rprocess),
       delta.t=1/12
       ),
     rmeasure=Csnippet(bsflu_rmeasure),
     dmeasure=Csnippet(bsflu_dmeasure),
     fromEstimationScale=Csnippet(bsflu_fromEstimationScale),
     toEstimationScale=Csnippet(bsflu_toEstimationScale),
     obsnames = bsflu_obsnames,
     statenames=bsflu_statenames,
     paramnames=bsflu_paramnames,
     initializer=Csnippet(bsflu_initializer)
) 
plot(bsflu)
```

To develop and debug code, it is nice to have a version that runs extra quickly, for which we set `run_level=1`. Here, `Np` is the number of particles (i.e., sequential Monte Carlo sample size), and `Nmif` is the number of iterations of the optimization procedure carried out below. Empirically, `Np=5000` and `Nmif=200` are around the minimum required to get stable results with an error in the likelihood of order 1 log unit for this example; this is implemented by setting `run_level=2`. One can then ramp up to larger values for more refined computations, implemented here by `run_level=3`.

```{r run_level}
run_level <- 1
switch(run_level,
       {bsflu_Np=100; bsflu_Nmif=10; bsflu_Neval=10; bsflu_Nglobal=10; bsflu_Nlocal=10}, 
       {bsflu_Np=20000; bsflu_Nmif=100; bsflu_Neval=10; bsflu_Nglobal=10; bsflu_Nlocal=10}, 
       {bsflu_Np=60000; bsflu_Nmif=300; bsflu_Neval=10; bsflu_Nglobal=100; bsflu_Nlocal=20}
)
```


### Running a particle filter

Before engaging in iterated filtering, it is often a good idea to check that the basic particle filter is working since iterated filtering builds on this technique. Here, carrying out slightly circular reasoning, we are going to test `pfilter` on a previously computed point estimate read in from [bsflu_params.csv](bsflu_params.csv):
 
```{r bsflu_params}
bsflu_params <- data.matrix(read.table("mif_bsflu_params.csv",row.names=NULL,header=TRUE))
bsflu_mle <- bsflu_params[which.max(bsflu_params[,"logLik"]),][bsflu_paramnames]
```

We are going to treat $\mu_{R_1}$ and  $\mu_{R_2}$ as known, fixed at the empirical mean of the bed-confinement and convalescence times for symptomatic cases:

```{r fixed_params}
bsflu_fixed_params <- c(mu_R1=1/(sum(bsflu_data$B)/512),mu_R2=1/(sum(bsflu_data$C)/512))
```

It is convenient to do some parallelization to speed up the computations. Most machines are multi-core nowadays, and using this computational capacity involves only (i) the following lines of code to let R know you plan to use multiple processors; (ii) using the parallel for loop provided by 'foreach'.

```{r pf1,cache=FALSE}
require(doParallel)
cores <- 4
registerDoParallel(cores)
mcopts <- list(set.seed=TRUE)

set.seed(396658101,kind="L'Ecuyer")
```

```{r bake,echo=FALSE,cache=FALSE}
bake <- function (file, expr) {
  if (file.exists(file)) {
    readRDS(file)
  } else {
    val <- eval(expr)
    saveRDS(val,file=file)
    val
  }
}
```

We proceed to carry out replicated particle filters at this tentative MLE:

```{r pf,cache=TRUE,cache.extra=list(run_level,rand_seed)}
t_pf <- system.time(
  pf <- foreach(i=1:20,.packages='pomp',
      .options.multicore=mcopts) %dopar% try(
    pfilter(bsflu,params=bsflu_mle,Np=bsflu_Np)
  )
)
(L_pf <- logmeanexp(sapply(pf,logLik),se=TRUE))
```

In  `r round(t_pf["elapsed"],1)` seconds, we obtain an unbiased likelihood estimate of `r round(L_pf[1],2)` with a Monte standard error of `r round(L_pf[2],2)`.


### A local search of the likelihood surface

Let's carry out a local search using `mif2` around this previously identified MLE. For that, we need to set the `rw.sd` and `cooling.fraction.50` algorithmic parameters:

```{r box_search_local,cache=TRUE,cache.extra=list(run_level,rand_seed)}
bsflu_rw.sd <- 0.02
bsflu_cooling.fraction.50 <- 0.5

t_local <- system.time({
  mifs_local <- foreach(i=1:bsflu_Nlocal,.packages='pomp', .combine=c, .options.multicore=mcopts) %dopar%  {
     mif2(
        bsflu,
        start=bsflu_mle,
        Np=bsflu_Np,
        Nmif=bsflu_Nmif,
        cooling.type="geometric",
        cooling.fraction.50=bsflu_cooling.fraction.50,
        transform=TRUE,
        rw.sd=rw.sd(
           beta=bsflu_rw.sd,
           mu_I=bsflu_rw.sd,
           rho=bsflu_rw.sd
        )
     )

  }
})
```

Although the filtering carried out by `mif2` in the final filtering iteration generates an approximation to the likelihood at the resulting point estimate, this is not usually good enough for reliable inference. Partly, this is because some parameter perturbations remain in the last filtering iteration. Partly, this is because `mif2` is usually carried out with a smaller number of particles than is necessary for a good likelihood evaluation---the errors in `mif2` average out over many iterations of the filtering. Therefore, we evaluate the likelihood, together with a standard error, using replicated particle filters at each point estimate:

```{r lik_local_eval,cache=TRUE,cache.extra=list(run_level,rand_seed)}
t_local_eval <- system.time({
  liks_local <- foreach(i=1:bsflu_Nlocal,.packages='pomp',.combine=rbind) %dopar% {
    evals <- replicate(bsflu_Neval, logLik(pfilter(bsflu,params=coef(mifs_local[[i]]),Np=bsflu_Np)))
    logmeanexp(evals, se=TRUE)
  }
})

results_local <- data.frame(logLik=liks_local[,1],logLik_se=liks_local[,2],t(sapply(mifs_local,coef)))
summary(results_local$logLik,digits=5)
```

This investigation took  `r round(t_local["elapsed"]/60,1)` minutes for the maximization and `r round(t_local_eval["elapsed"]/60,1)` minutes for the likelihood evaluation. These repeated stochastic maximizations can also show us the geometry of the likelihood surface in a neighborhood of this point estimate:

```{r pairs_local}
pairs(~logLik+beta+mu_I+rho,data=subset(results_local,logLik>max(logLik)-50))
```

### A global search of the likelihood surface using randomized starting values

When carrying out parameter estimation for dynamic systems, we need to specify beginning values for both the dynamic system (in the state space) and the parameters (in the parameter space). By convention, we use  _initial values_ for the initialization of the dynamic system and _starting values_ for initialization of the parameter search.

Practical parameter estimation involves trying many starting values for the parameters. One can specify a large box in parameter space that contains all parameter vectors which seem remotely sensible. If an estimation method gives stable conclusions with starting values drawn randomly from this box, this gives some confidence that an adequate global search has been carried out. 

For our flu model, a box containing reasonable parameter values might be

```{r box}
bsflu_box <- rbind(
  beta=c(0.001,0.01),
  mu_I=c(0.5,2),
  rho = c(0.5,1)
)
```


We are now ready to carry out likelihood maximizations from diverse starting points. To simplify the code, we can reset only the starting parameters from `mifs_global[[1]]` since the rest of the call to `mif2` can be read in from `mifs_global[[1]]`:

```{r box_eval,cache=TRUE,cache.extra=list(run_level,rand_seed)}
t_global <- system.time({
  mifs_global <- foreach(i=1:bsflu_Nglobal,.packages='pomp', .combine=c, .options.multicore=mcopts) %dopar%  mif2(
     mifs_local[[1]],
     start=c(apply(bsflu_box,1,function(x)runif(1,x)),bsflu_fixed_params)
  )
})
```

Although the filtering carried out by `mif2` in the final filtering iteration generates an approximation to the likelihood at the resulting point estimate, this is not usually good enough for reliable inference. Partly, this is because some parameter perturbations remain in the last filtering iteration. Partly, this is because `mif2` is usually carried out with a smaller number of particles than is necessary for a good likelihood evaluation---the errors in `mif2` average out over many iterations of the filtering. Therefore, we evaluate the likelihood, together with a standard error, using replicated particle filters at each point estimate:

```{r lik_global_eval,cache=TRUE,cache.extra=list(run_level,rand_seed)}
t_global_eval <- system.time({
  liks_global <- foreach(i=1:bsflu_Nglobal,.packages='pomp',.combine=rbind, .options.multicore=mcopts) %dopar% {
    evals <- replicate(bsflu_Neval, logLik(pfilter(bsflu,params=coef(mifs_global[[i]]),Np=bsflu_Np)))
    logmeanexp(evals, se=TRUE)
  }
})

results_global <- data.frame(logLik=liks_global[,1],logLik_se=liks_global[,2],t(sapply(mifs_global,coef)))
summary(results_global$logLik,digits=5)
```

It is good practice to build up a file of successful optimization results for subsequent investigation:

```{r save_params}
if(run_level>1) write.table(rbind(results_local,results_global),
   file="mif_bsflu_params.csv",append=TRUE,col.names=FALSE,row.names=FALSE)
```

Evaluation of the best result of this search gives a likelihood of `r round(max(results_global$logLik),1)` with a standard error of `r round(results_global$logLik_se[which.max(results_global$logLik)],1)`. This took in `r round(t_global["elapsed"]/60,1)` minutes for the maximization and `r round(t_global_eval["elapsed"]/60,1)` minutes for the evaluation.  Plotting these diverse parameter estimates can help to give a feel for the global geometry of the likelihood surface 

```{r pairs_global}
pairs(~logLik+beta+mu_I+rho,data=subset(results_global,logLik>max(logLik)-250))
```

We see that optimization attempts from diverse remote starting points end up with comparable likelihoods, even when the parameter values are quite distinct. This gives us some confidence in our maximization procedure. 

## Exercises

### Technical exercise: Construct a profile likelihood.

How strong is the evidence about the contact rate, $\beta$, given this model and data? Use `mif2` to construct a profile likelihood. Due to time constraints, you may be able to compute only a preliminary version.

It is also possible to profile over the basic reproduction number, $R_0=\beta P/\mu_I$. Is this more or less well determined that $\beta$ for this model and data?

### Technical exercise: Check the model source code.

Check the source code for the `bsflu` pomp object. Does the code implement the model described?

For various reasons, it can be surprisingly hard to make sure that the written equations and the code are perfectly matched. Here are some things to think about:

1. Papers should be written to be readable to as broad a community as possible. Code must be written to run successfully. People do not want to clutter papers with numerical details which they hope and belief are scientifically irrelevant. What problems can arise due to this, and what solutions are available?

2. Suppose that there is an error in the coding of `rprocess`. Suppose that plug-and-play statistical methodology is used to infer parameters.  A conscientious researcher carries out a simulation study, using `simulate` to generate some realizations from the fitted model and checking that the inference methodology can successfully recover the known parameters for this model, up to some statistical error. Will this procedure help to identify the error in `rprocess`? If not, how does one debug `rprocess`? What research practices help minimize the risk of errors in simulation code?

### Technical exercise: Assessing and improving algorithmic parameters

Develop your own heuristics to try to improve the performance of `mif2` in the previous example. Specifically, for a global optimization procedure carried out using random starting values in the specified box, let $\hat\Theta_\mathrm{max}$ be a random Monte Carlo estimate of the resulting MLE, and let $\hat\theta$ be the true (unknown) MLE. We can define the maximization error in the log likelihood to be
$$e = \ell(\hat\theta) - E[\ell(\hat\Theta_\mathrm{max})].$$
We cannot directly evaluate $e$, since there is also Monte Carlo error in our evaluation of $\ell(\theta)$, but we can compute it up to a known precision. Plan some code to estimates $e$ for a search procedure using a computational effort of $JM=2\times 10^7$, comparable to that used for each mif computation in the global search. Discuss the strengths and weaknesses of this quantification of optimization success. See if you can choose $J$ and $M$ subject to this constraint, together with choices of `rw.sd` and the cooling rate, `cooling.fraction.50`, to arrive at a quantifiably better procedure. Computationally, you may not be readily able to run your full procedure, but you could run a quicker version of it.

### Technical exercise: Finding sharp peaks in the likelihood surface

Even in this small, 3 parameter, example, it takes a considerable amount of computation to find the global maximum (with values of $\beta$ around 0.004) starting from uniform draws in the specified box. The problem is that, on the scale on which "uniform" is defined, the peak around $\beta\approx 0.004$ is very narrow. Propose and test a more favorable way to draw starting parameters for the global search, with better scale invariance properties.

```{r save, include=FALSE}
if(run_level>1) save(list = ls(all = TRUE), file = "Rout.rda")
```

### Technical exercise: Adding a latent class

Modify the model to include a latent period between becoming exposed and becoming infectious. See what effect this has on the maximized likelihood.

----------------------


## References





